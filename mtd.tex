\documentclass[aps,prc,reprint,amsmath]{revtex4-1}

\usepackage{amssymb}
\usepackage[bookmarksopen=true]{hyperref}
\usepackage{graphicx}
\graphicspath{{fig/}}

\usepackage{color}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\usepackage{multirow}

\newcommand{\colfig}[3][t]{
  \begin{figure}[#1]
    \includegraphics{#2}
    \caption{\label{fig:#2}#3}
  \end{figure}
}

\newcommand{\widefig}[3][t]{
  \begin{figure*}[#1]
    \includegraphics{#2}
    \caption{\label{fig:#2}#3}
  \end{figure*}
}

\newcommand{\placeholderfig}[3][t]{
  \begin{figure}[#1]
    \centering
    \framebox{\parbox[c][.5\columnwidth]{\columnwidth}{
      placeholder
    }}
    \caption{\label{fig:#2}#3}
  \end{figure}
}

\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\nch}{N_\text{ch}}
\newcommand{\vnk}[2]{v_#1\{#2\}}
\newcommand{\tran}{^\intercal}
\newcommand{\trento}{T\raisebox{-.5ex}{R}ENTo}


\begin{document}

\title{Extracting QGP properties via systematic model-to-data comparison}

\author{Jonah E.\ Bernhard}
\author{Steffen A.\ Bass}
\author{Christopher E.\ Coleman-Smith}
\author{Robert L.\ Wolpert}
\affiliation{Duke University}

\author{Snehalata Huzurbazar}
\affiliation{University of Wyoming}

\author{Peter Marcy}
\affiliation{Los Alamos National Lab}


\date{\today}

\begin{abstract}
  An event-by-event heavy-ion collision model is systematically tested on a massive scale.
  We calculate anisotropic flows and charged-particle multiplicities over wide ranges of model parameters, including fundamental QGP properties such as the specific shear viscosity $\eta/s$, and use a Bayesian procedure to calibrate the model to experimental data and extract quantitative constraints for all model parameters.
\end{abstract}

\maketitle


\section{Introduction}

Relativistic heavy-ion collisions produce a hot, dense phase of strongly-interacting matter commonly known as the quark-gluon plasma (QGP) which rapidly expands and freezes into discrete particles \cite{Arsene:2004fa,Adcox:2004mh,Back:2004je,Adams:2005dq,Gyulassy:2004zy,Muller:2006ee,Muller:2012zq}.
Since the QGP is not directly observable---only final-state hadrons are detected---present research seeks to quantify the fundamental properties of the QGP, such as its transport coefficients and the nature of the initial state, through comparisons of experimental measurements to computational model calculations.

Computational models must take a set of input parameters including the physical properties of interest, simulate the full time-evolution of heavy-ion collisions, and produce output analogous to experimental measurements.
The true values of the physical properties are extracted by calibrating the input parameters so that the model output optimally reproduces experimental data.
This generic recipe is called ``model-to-data comparison''.

Notably, previous studies have used viscous relativistic fluid dynamics to place constraints on the QGP shear viscosity to entropy density ratio $\eta/s$ by comparing anisotropic flow coefficients $v_n$ between model and experiment.
$\eta/s$ cannot be calculated directly from QCD, and while there is a conjectured lower bound $\eta/s~\geq~1/4\pi~\simeq~0.08$ from AdS/CFT holography \cite{Kovtun:2004de}, model-to-data comparison is the most attractive option for determining the precise value.
These studies generally proceed by calculating fluid dynamical $v_n$ at several values of $\eta/s$, then choosing the value which most closely matches experimental $v_n$.
A variety of complementary results have constrained $\eta/s$ to an approximate range of 0.08--0.20 \cite{Luzum:2008cw,Song:2010mg,Schenke:2010rr}.

However, $\eta/s$ is not the only model input parameter:
many other parameters remain unconstrained, e.g.~the hydrodynamic thermalization time $\tau_0$, equation of state, and initial conditions; and models often have non-physical nuisance parameters that nonetheless should be tuned to optimal values.
$v_n$ is not the only output:
basic observables like the charged-particle multiplicity and transverse-momentum distributions must also be simulated accurately.
Input parameters are in general correlated among each other and each affects multiple outputs, so they cannot be constrained independently.
Due to these practical limitations, most studies to date have focused on a single input parameter, fixed secondary parameters to default values, and calibrated to a minimal set of observables.
This leads to nebulous results with poorly-defined constraints, non-quantitative uncertainties, and no treatment of correlations.

\todo{
  Cite \cite{Luzum:2012wu} and \cite{Soltz:2012rk}.
  They do more systematic studies but do not use an emulator.
}

Algorithms such as Markov chain Monte Carlo (MCMC) can rigorously explore this type of complex multivariate input-output parameter space, but MCMC requires a very large number of model evaluations: $\mathcal O(10^6)$ or more.
Heavy-ion collision models may run for several hours, so a direct MCMC approach is intractable.
The situation is exacerbated with a full event-by-event model, since many thousands of events must be calculated \emph{at each point in parameter space} to capture event-by-event fluctuations.

These limitations may be overcome through a modern Bayesian method for analyzing computationally expensive models \cite{OHagan:2006ba,Higdon:2008cmc,Higdon:2014tva}.
A set of salient model parameters is chosen for calibration---the set should include any fundamental physical properties of interest---and the model is evaluated at a relatively small $\mathcal O(10^2)$ number of points in parameter space.
Those points are then interpolated with a Gaussian process emulator \cite{Rasmussen:2006gp} to provide a continuous picture of parameter space.
The emulator acts as a fast surrogate to the full model:
it predicts model output at arbitrary points in parameter space with negligible computational cost.
Parameters are calibrated by sampling the emulator with standard MCMC techniques.

Emulators have been successfully used to study a wide range of physical systems, including galaxy formation \cite{Gomez:2012ak} and heavy-ion collisions \cite{Novak:2013bqa}.
Reference \cite{Novak:2013bqa} calibrated a hydrodynamic model to identified particle spectra from the Relativistic Heavy Ion Collider (RHIC) and extracted constraints on $\eta/s$ and several initial state parameters.
However, this study used an event-averaged initial condition model, limiting its ability to investigate event-by-event fluctuations.

In this work, we apply Bayesian methodology to a full event-by-event heavy-ion collision model.
We calibrate to anisotropic flow data from the Large Hadron Collider (LHC) and constrain the shear viscosity $\eta/s$ along with other hydrodynamic and initial condition parameters.
The analysis framework handles arbitrary numbers of inputs and outputs, systematically calculates quantitative constraints on all inputs simultaneously, and quickly evaluates the efficacy of physical models.


\section{Model}

Prototypical heavy-ion collision models simulate QGP spacetime evolution in several stages:
an initial condition model handles the very early pre-QGP stage, viscous relativistic hydrodynamics simulates the hot and dense QGP expansion, the system is converted into an ensemble of hadrons, and finally a Boltzmann transport model calculates hadronic scatterings and decays \cite{Bass:2000ib,Nonaka:2006yn,Song:2010mg}.

We opt for a mature, well-tested set of event-by-event models \cite{Shen:2014vra}.
This way, the outcome of the systematic model-to-data comparison will be directly comparable to known results using the same models.

\subsection{Initial conditions}

Initial condition models simulate the collision's pre-equilibrium evolution until the hydrodynamic thermalization time at approximately 0.5 fm/c.
Some models explicitly calculate pre-equilibrium dynamics \cite{}; others skip this time frame and generate initial conditions directly at the thermalization time.

We select two of the oldest and most widely used models in the latter category:
the Monte Carlo Glauber \cite{Miller:2007ri} and Monte Carlo KLN \cite{Drescher:2006pi} models.
Although neither is state-of-the-art, both provide reasonable event-by-event initial conditions with well-understood behavior.

\subsection{Hydrodynamics}

The initial condition furnishes the hydrodynamic stress-energy tensor $T^{\mu\nu}$ at the thermalization time $\tau_0$.
Viscous hydrodynamics then solves the conservation equations
\begin{equation}
  \partial_\mu T^{\mu\nu} = 0
\end{equation}
where
\begin{equation}
  T^{\mu\nu} = (\epsilon + P) u^\mu u^\nu - P g^{\mu\nu} + \pi^{\mu\nu};
\end{equation}
$\epsilon$, $P$, and $u^\mu$ are the energy density, pressure, and flow velocity of the fluid; $g^{\mu\nu}$ is the metric tensor; and $\pi^{\mu\nu}$ is the shear stress tensor.
An equation of state
\begin{equation}
  P = P(\epsilon)
\end{equation}
closes the system of hydrodynamic equations and is usually provided by lattice QCD calculations.

We employ VISH2+1 \cite{Song:2007ux}, a stable, extensively tested implementation of boost-invariant viscous hydrodynamics \cite{Bjorken:1982qr}.
VISH2+1 uses the popular s95-PCE equation of state \cite{Huovinen:2009yb}.

\subsection{Freeze-out}

As the medium expands and cools, it freezes into discrete hadrons with spectra given by the Cooper-Frye formula \cite{Cooper:1974mv}
\begin{equation}
  E \frac{dN_i}{d^3p} = \int_\sigma f_i(x,p) \, p^\mu \, d^3\sigma_\mu,
  \label{eq:cf}
\end{equation}
where $f_i$ is the distribution function for particle species $i$, $p^\mu$ is the four-momentum, and the integral is taken over an isothermal spacetime hypersurface $\sigma$ at the equation of state transition temperature $T \simeq 165$ MeV.
Equation~\eqref{eq:cf} is randomly sampled as a probability density function to produce an ensemble of particles.

We use a recent hypersurface sampler designed specifically for an event-by-event context \cite{Qiu:2013wca,Shen:2014vra}.

\subsection{Afterburner}

After freeze-out, the medium continues to expand as a hadron gas while particles scatter and decay.
This stage is modeled by a hadronic ``afterburner'', which solves the Boltzmann equation
\begin{equation}
  \frac{df_i(x,p)}{dt} = \mathcal C_i(x,p),
\end{equation}
where $f_i$ is the distribution function for particle species $i$ and $\mathcal C_i$ is the collision kernel which describes source terms.
Particles emerging from the afterburner are analogous to particles streaming into an experimental detector.

We adopt the stalwart Ultra-relativistic Quantum Molecular Dynamics (UrQMD) model \cite{Bass:1998ca,Bleicher:1999xi} as an afterburner.

\subsection{Postprocessing}

The full event-by-model is executed $\mathcal O(10^4)$ times for a given set of input parameters.
Events are binned into centrality intervals and the raw event data are postprocessed into physical observables for direct comparison with experiment.
We calculate the centrality dependence of several standard standard observables:
the average charged-particle multiplicity $\avg\nch$ and multi-particle flow cumulants $\vnk n {2k}$.

Flow cumulants $\vnk n {2k}$ are defined as the $2k$-particle correlation function of the $n$th-order azimuthal anisotropy.
For example, the two-particle cumulant is
\begin{equation}
  \vnk n 2^2 \equiv \bigl\langle e^{in(\phi_i - \phi_j)} \bigr\rangle,
\end{equation}
where $\phi_i$ is the azimuthal angle of the transverse momentum of particle $i$ and the average is over all distinct pairs of particles $i,j$.
The two-particle cumulant is also approximately equal to the root-mean-square of the full $v_n$ distribution:
\begin{equation}
  \vnk n 2 \simeq \sqrt{\avg{v_n^2}}.
\end{equation}
We compute two-particle cumulants for elliptic and triangular flow $\vnk 2 2$, $\vnk 3 2$ using the direct $Q$-cumulant method \cite{Bilandzic:2010jr}.
Higher-order cumulants are currently out of reach due to insufficient quantities of events.

Postprocessed observables are compared to corresponding experimental results recently measured by the ALICE experiment at the LHC \cite{Abelev:2014mda}.
All observables are subjected to the same kinematic cuts as the ALICE detector, namely charged particles with $|\eta| < 1$ and $0.2 < p_T < 3.0$ GeV.


\section{Emulator}

This section constructs a Gaussian process (GP) emulator to act as a surrogate for the full event-by-event model.
The strategy is to evaluate the model on a carefully chosen set of input parameter points, then use a GP to interpolate the parameter space.
Unlike alternative interpolation schemes such as splines or polynomial interpolation, a GP emulator provides a \emph{probability distribution} at each point in parameter space, hence, it not only predicts the output of the model at arbitrary points in parameter space, but also quantifies the uncertainty of its predictions.
Further, GPs are non-parametric interpolators, i.e.\ they do not require an assumed functional form for the underlying model.
These features are essential for emulation of computer codes.

\subsection{Gaussian processes}

\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\zero}{\mathbf 0}
\newcommand{\muvec}{\boldsymbol\mu}
\newcommand{\N}{\mathcal N}

This subsection summarizes the theory of Gaussian process emulators as detailed in chapter 2 of \cite{Rasmussen:2006gp}.

A Gaussian process (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution.
A GP may be thought of as a stochastic function $f(\x)$ which maps $n$-dimensional input vectors $\x$ to normally distributed outputs $y$.
It is fully specified by a mean function $\mu(\x)$ which gives the mean of $f$ at input point $\x$ and a covariance function $\sigma(\x, \x')$ which provides the covariance of $f$ between a pair of points $\x$, $\x'$.

As a concrete example, let $\x_1$ be an input point and $y_1 = f(\x_1)$ be the output of the GP at $\x_1$; then $y_1$ has a normal distribution with mean $\mu(\x_1)$ and variance $\sigma(\x_1, \x_1)$:
\begin{equation}
  y_1 \sim \N(\mu(\x_1), \sigma(\x_1, \x_1)).
\end{equation}
Now if $\x_2$ is another input and $y_2 = f(\x_2)$ is the corresponding output, $y_1$ and $y_2$ have a bivariate normal distribution
\begin{equation}
  \begin{pmatrix}
    y_1 \\ y_2
  \end{pmatrix}
  \sim \N\biggl[
    \begin{pmatrix}
      \mu(\x_1) \\ \mu(\x_2)
    \end{pmatrix},
    \begin{pmatrix}
      \sigma(\x_1, \x_1) & \sigma(\x_1, \x_2) \\
      \sigma(\x_2, \x_1) & \sigma(\x_2, \x_2) \\
    \end{pmatrix}
  \biggr].
\end{equation}
In general, the set of $m$ random output variables $\y~=~\{y_1, \ldots, y_m\}~=~f(X)$ corresponding to input points $X~=~\{\x_1, \ldots, \x_m\}$ have a multivariate normal distribution
\begin{equation}
  \y \sim \N(\muvec, \Sigma)
\end{equation}
where
\begin{equation}
  \muvec = \mu(X) = \{\mu(\x_1), \mu(\x_2), \ldots, \mu(\x_m)\} \\
\end{equation}
is the $m$-dimensional mean vector from applying the mean function to each input, and
\begin{equation}
  \Sigma = \sigma(X, X) =
  \begin{pmatrix}
    \sigma(\x_1, \x_1) & \cdots & \sigma(\x_1, \x_m) \\
    \vdots & \ddots & \vdots \\
    \sigma(\x_m, \x_1) & \cdots & \sigma(\x_m, \x_m) \\
  \end{pmatrix}
\end{equation}
is the $m \times m$ covariance matrix from applying the covariance function to each pair of inputs..

In practice, one often sets the mean function to zero, since one can always subtract off the mean of a distribution.
The covariance function must be carefully chosen, for it controls the degree of similarity between pairs of points.
A standard choice is the squared-exponential function
\begin{equation}
  \sigma(\x, \x') = \exp\biggl( -\frac{|\x - \x'|^2}{2\ell^2} \biggr),
\end{equation}
where $\ell$ is a characteristic length scale.
Notice that nearby points are strongly correlated ($\sigma \approx 1$) while distant points are independent ($\sigma \rightarrow 0$).
This implies that the GP is smooth, i.e.\ nearby input points produce similar outputs.

Just as we can sample random numbers from a distribution, we can draw random functions from a GP.
We choose a set of test points $X_*$ (the reason for the subscript $*$ will become clear in a moment), calculate the covariance matrix $\Sigma = \sigma(X_*, X_*)$, and generate multivariate normal samples from $\N(\zero, \Sigma)$.
We can then plot the input-output points as smooth curves, as in the top panel of FIG.~\ref{fig:gp}.

\colfig{gp}{
  Top: random functions drawn from a Gaussian process using a squared-exponential covariance function with length scale $\ell = 1$.
  Bottom: functions drawn from a GP conditioned on the training points indicated by dots.
  In both plots, the dashed line represents the GP mean and the grey band is twice the GP standard deviation (roughly 95\% confidence interval).
}

Of course, simply generating random functions is not particularly useful---we want to use a GP to interpolate a computer model.
Suppose we have a model which takes a vector of input parameters $\x$ and produces an output $y$ according to some unknown GP $f(\x)$; for example, $f$ could be a hydrodynamic model with input parameters $\x = (\tau_0, \eta/s)$ and the output could be elliptic flow $v_2$.
We choose a set of training points $X$, run the model at each point, and observe a set of outputs $\y$.
Now, instead of completely random functions, we desire functions which pass through (interpolate) all the training points $(X, \y)$.
This is accomplished by \emph{conditioning} the GP on the training data to yield a predictive distribution for $y$ at any input point $\x$.
Recalling the test points $X_*$, the predictive distribution for the corresponding outputs $\y_*$ is the multivariate normal distribution
\begin{equation}
  \begin{aligned}
    \y_* &\sim \N(\muvec, \Sigma), \\
    \muvec &= \sigma(X_*, X)\sigma(X, X)^{-1}\y, \\
    \Sigma &= \sigma(X_*,X_*) - \sigma(X_*,X)\sigma(X,X)^{-1}\sigma(X,X_*).
  \end{aligned}
  \label{eq:cond}
\end{equation}
See the bottom panel of FIG.~\ref{fig:gp} for an example of conditioning a GP on one-dimensional training data.

We emphasize that the prediction $\y_*$ is not constant, but a \emph{probability distribution} for the model outputs at $X_*$.
As demonstrated in FIG.~\ref{fig:gp}, the predictive distribution is narrow when near the training points and wide when far away, hence, it reflects the true state of knowledge of the interpolation.
This is accomplished without assuming a parametric form for the model---we must only assume that the model is a GP with a specified covariance function.


\subsection{Computer experiment design}

\colfig[b]{design}{
  The Latin hypercube experiment design projected into the $(\eta/s, \tau_0)$ dimensions.
  All other parameters also vary across the design, so the points that appear very close in the projection are not necessarily close in the full-dimensional space.
  The edge histograms show the distributions flattened into one dimension; note that they are approximately uniform.
}

\widefig{prior_draws}{
  Prior model calculations using Glauber (top, blue) and KLN (bottom, green) initial conditions.
  From left to right:
  average charged-particle multiplicity $\avg\nch$,
  elliptic flow two-particle cumulant $\vnk 2 2$,
  and triangular flow two-particle cumulant $\vnk 3 2$.
  Each plot has 254 lines corresponding to the 254 design points.
  Data points are experimental measurements from ALICE \cite{Abelev:2014mda}.
}

The full event-by-event model is to be evaluated on a set of training points $X = \{\x_1, \ldots, \x_m\}$, where each $\x_i$ is an $n$-dimensional vector of input parameters, so $X$ may be viewed as an $m \times n$ design matrix.
These points must be chosen with care.

For the present study, we choose a set of $n = 5$ input parameters
\begin{equation}
  \x = (\text{Norm}, \text{I.C.\ param}, \tau_0, \eta/s, \tau_\Pi)
\end{equation}
where
\begin{itemize}
  \item Norm is the overall normalization factor, a multiplicative constant that determines how much entropy is deposited in the initial condition.
  \item I.C.\ param is a parameter specific to each initial condition model.
    For the Glauber model the parameter is $\alpha$, which controls how entropy is distributed to wounded nucleons and binary collisions;
    for the KLN model it is $\lambda$, the exponent for the saturation scale.
    Both parameters are related to the centrality dependence of multiplicity.
    \todo{Explain more or introduce earlier?}
  \item $\tau_0$ is the hydrodynamic thermalization time.
  \item $\eta/s$ is the shear viscosity to entropy density ratio.
  \item $\tau_\Pi$ is the shear stress relaxation time, which dictates how quickly the hydro medium relaxes to the Navier-Stokes limit.
    \todo{Verify this.}
\end{itemize}
We choose intentionally large ranges for each parameter, summarized in table~\ref{tab:design}.

The number $m$ of design points and their distribution in parameter space is also critical.
Perhaps the most obvious choice is a uniform grid, e.g.\ $k$ evenly-spaced points in each dimension.
Unfortunately, this leads to a total number of design points $m = n^k$ which even for a modest $k = 10$ and $n = 5$ is intractably large, $\mathcal O(10^7)$.

\begin{table}[b]
  \caption{
    \label{tab:design}
    Input parameter ranges for the Glauber (Glb) and KLN initial condition models and for the hydrodynamic model.
  }
  \begin{ruledtabular}
  \begin{tabular}{lll}
    \bf Parameter & \bf Description & \bf Range \\
    Glb Norm & Overall normalization & 20--60 \\
    Glb $\alpha$ & Wounded nucleon / binary collision & 0.05--0.30 \\
    KLN Norm & Overall normalization & 5--15 \\
    KLN $\lambda$ & Saturation scale exponent & 0.1--0.3 \\
    $\tau_0$ & Thermalization time & 0.2--1.0 fm \\
    $\eta/s$ & Specific shear viscosity & 0--0.3 \\
    $\tau_\Pi$ & Shear stress relaxation time & 0.2--1.1 fm \\
  \end{tabular}
  \end{ruledtabular}
\end{table}

A popular algorithm for generating efficient design points is Latin hypercube (LH) sampling \cite{Tang:1993lh}.
This method produces space-filling randomized designs with several desirable properties:
\begin{itemize}
  \item The \emph{minimum} distance between points is \emph{maximized}, thus avoiding large gaps and tight clusters.
  \item Projections of the design into lower dimensions are uniformly distributed.
\end{itemize}
Figure~\ref{fig:design} illustrates these traits.
A LH design with relatively few points $m \sim 20n$ provides an efficient scaffolding of parameter space for interpolation by a GP emulator.

We use a 256 point LH design across the $n = 5$ input parameters; FIG.~\ref{fig:design} shows a two-dimensional projection.
At each design point, we have executed the event-by-event model $\mathcal O(10^3)$ times in each of six centrality bins 0--5\%, 10--15\%, \ldots, 50-55\%, for both the Glauber and KLN models, yielding $\mathcal O(10^7)$ events in total.
Two design points that were very near to the edge of the design space resulted in non-physical results and have been discarded, so the operational design has $m = 254$ points.
\todo{Mention the OSG?}

Figure~\ref{fig:prior_draws} shows the postprocessed observables $\avg\nch$, $\vnk 2 2$, $\vnk 3 2$ as a function of centrality for each point in the design.
The results have a broad distribution which is a direct result of the wide ranges of input parameters.
There is some statistical error present in $\vnk 3 2$ due to insufficient quantities of events.

Note that these results constitute the training data for the GP emulator, not any kind of best-fit.

\subsection{Multivariate output}

\colfig[b]{pc_scatter}{
  Principal component decomposition of the observables $\sqrt{\avg\nch}$, $\vnk 2 2$ for the Glauber model in 20--25\% centrality.
  Each data point represents a model calculation and the edge histograms show the approximate normal distribution of each observable.
  Arrows represent the PC vectors with lengths proportional to the explained variance.
}

Gaussian processes are fundamentally scalar functions, but computer models often produce multivariate output.
In general, the model takes the $m \times n$ design matrix $X$ and computes an $m \times p$ output matrix $Y$.
The present event-by-event model has $p = 18$ outputs (three observables each in six centrality bins).

An obvious workaround is to use independent GP emulators for each of the $p$ outputs, however, this would neglect correlations and quickly become unwieldy for higher-dimensional output.
Instead, we construct orthogonal linear combinations of the outputs via principal component (PC) analysis and emulate each transformed PC.
The PCs are uncorrelated by construction and can also be used to reduce the dimensionality of the output space.

To construct the PCs, we first subtract the mean of the output data $Y$ so that each column has mean zero, then compute the eigendecomposition of the sample covariance matrix $Y\tran Y$:
\begin{equation}
  Y\tran Y = U \Lambda U\tran,
  \label{eq:cov}
\end{equation}
where $U$ is an orthogonal $p \times p$ matrix containing the eigenvectors of $Y\tran Y$ and $\Lambda$ is diagonal containing the eigenvalues $\lambda_1, \ldots, \lambda_p$ in non-increasing order.
$U$ now defines a linear transformation which ``rotates'' the output data $Y$ into PC space:
\begin{equation}
  Z = \sqrt m \, YU,
\end{equation}
where $Z$ is an $m \times p$ matrix (same shape as $Y$) of the transformed PCs.
The eigenvalues $\lambda_i$ represent the variance accounted for by principal component $i$; since they are sorted in non-increasing order, the \emph{fraction} of the variance accounted for by the first $q \leq p$ PCs is
\begin{equation}
  V(q) = \frac{\sum_{i=1}^q \lambda_i}{\sum_{i=1}^p \lambda_i}.
\end{equation}
Often, the first few PCs describe most of the variance, as shown in FIG.~\ref{fig:pc_var}.
Hence we can construct a reduced-dimension transformation with minimal loss of precision by choosing $q < p$ so that $V(q)$ satisfies some threshold (e.g.~$V(q) \geq 0.99$) and taking only the first $q$ columns of $U$:
\begin{equation}
  Z_q = \sqrt m \, YU_q,
\end{equation}
where $Z_q$ is now an $m \times q$ matrix.

\colfig{pc_var}{
  Fraction of the variance $V(q)$ explained by the first $q$ principal components for Glauber (blue circles) and KLN (green triangles).
  $q = 5$ accounts for approximately 99\% of the total variance, a significant reduction from the original 18 dimensions.
}

We may now use $q$ independent GP emulators for each of the columns of $Z_q$.
GPs are conditioned on the design $X$ according to Eq.~\eqref{eq:cond} and predict the PCs $Z_*$ at arbitrary test points $X_*$ which are then transformed back to physical space as
\begin{equation}
  Y_* = \frac{1}{\sqrt m} Z_* U\tran.
\end{equation}

There is an important caveat for principal components:
the original data $Y$ must have a multivariate normal distribution for the transformed PCs $Z$ to be uncorrelated.
There is no guarantee that a particular model will produce normally-distributed outputs so this must be verified on a case-by-case basis.
For the present event-by-event model we perform the following steps:
\begin{enumerate}
  \item Assess the normality of each observable $\avg\nch$, $\vnk 2 2$, $\vnk 3 2$.
    While the flow cumulants are approximately normal without modification, we take the square root of multiplicity $\sqrt{\avg\nch}$ to obtain a normal distribution.
  \item Divide each observable by its corresponding experimental value from ALICE \cite{Abelev:2014mda}.
    This converts everything to unitless quantities of order one.
  \item \todo{Weights?  The current results have $\avg\nch$~:~$\vnk 2 2$~:~$\vnk 3 2$ weighted 1.2~:~1.0~:~0.6.}
  \item Concatenate the unitless data into a $254 \times 18$ matrix $Y$ and subtract the mean.
  \item Calculate the PC transformed data $Z$ using $q = 5$ PCs, which accounts for over 99\% of the variance as shown in FIG.~\ref{fig:pc_var}.
\end{enumerate}
These steps are inverted to transform PCs back to physical space.

\widefig{validation}{
  Validation of the Gaussian process emulator for the Glauber model.
  Each plot shows emulator predictions against explicit calculations for the 64 validation design points and centrality bins 0--5\% (green), 20--25\% (orange), and 40-45\% (purple).
  The $x$-value of each data point is the emulator prediction with 95\% error bars; the $y$-value is the explicit calculation.
  The diagonal grey line represents $y = x$.
}

In practice, the covariance method for computing principal components is prone to numerical error, so a more robust algorithm using the singular value decomposition (SVD) is preferred.
The SVD of the data $Y$ is
\begin{equation}
  Y = VDW\tran
  \label{eq:svd}
\end{equation}
where $V$, $W$ are orthogonal matrices containing the so-called left and right-singular vectors of $Y$ and $D$ is diagonal containing the singular values.
Inserting \eqref{eq:svd} into \eqref{eq:cov} yields
\begin{equation}
  Y\tran Y = W D^2 W\tran = U \Lambda U\tran,
\end{equation}
hence the singular values $D$ are the square root of the eigenvalues $\Lambda$ and the right singular vectors $W$ are the eigenvectors $U$.

\subsection{Emulator construction and validation}

We emulate the model by conditioning independent Gaussian processes on each of the principal components $Z_q$ and the input design $X$ according to Eq.~\eqref{eq:cond}.
Model outputs inevitably include statistical noise, i.e.\ we cannot compute $y = f(\x)$ exactly, only $y = f(\x) + \epsilon$ where $\epsilon$ is Gaussian noise.
This is accounted for by adding a noise term to the diagonal of the covariance matrix:
\begin{equation*}
  \sigma(\x, \x') \rightarrow \sigma(\x, \x') + \sigma^2_n\delta_{\x\x'},
\end{equation*}
where $\sigma^2_n$ is the variance of the noise and $\delta_{\x\x'}$ is a Kronecker delta.
Effectively, the noise term relaxes the requirement that the GP must pass exactly through each training point.

We use a squared-exponential covariance function with a noise term:
\begin{equation}
  \sigma(\x, \x') = \sigma_\text{GP}^2 \exp\Biggl[ -\sum_{k=1}^n \frac{(x_k - x'_k)^2}{2\ell_k^2} \Biggr] + \sigma_n^2\delta_{\x\x'},
\end{equation}
where $\sigma_\text{GP}^2$ is the overall variance of the GP and $\ell_k$ is the characteristic length scale for dimension $k$.
These \emph{hyperparameters} $(\sigma_\text{GP}$, $\sigma_n$, $\ell_k)$ are not known a priori and must be estimated from the training data, however, in the present case predictions appear to be relatively insensitive to the precise choice of hyperparameters, as will be demonstrated promptly.
For details about the selection of hyperparameters see appendix \ref{app:train}.

As with any interpolation scheme, the GP emulator must be validated to ensure it faithfully predicts model output.
In other words, given an arbitrary test point $\x_*$, the GP prediction at $\x_*$ should agree (within its uncertainty) with an explicit computation at $\x_*$.
To this end, we have generated a separate validation design $X_*$, evaluated the full event-by-event model at each validation point just as for the training design $X$, and predicted the model outputs at $X_*$ using the GP emulator.

Figure~\ref{fig:validation} validates that the emulator does indeed faithfully predict the model.
Recall that the emulator provides probability distributions of finite width, so it need not predict every validation point exactly---in fact, in the ideal case the residuals would have a normal distribution with mean zero.
Most of the uncertainty visible in FIG.~\ref{fig:validation} is actually due to the statistical noise in the flow cumulants, particularly for $\vnk 3 2$.
The emulator accurately depicts the noise present in the underlying data.
\todo{Additional figure showing the distribution of emulator errors?}


\section{Calibration}

\newcommand{\xs}{\x_\star}
\newcommand{\z}{\mathbf z}
\newcommand{\yexp}{\y_\text{exp}}
\newcommand{\zexp}{\z_\text{exp}}

With the validated Gaussian process emulator in hand, it may be used as a fast surrogate to the full event-by-event model for calibration.
\emph{Calibration} means tuning the model input parameters so that the output optimally agrees with experimental data and in the process extracting probability distributions for each parameter.
Recall the input parameters are
\begin{equation*}
  \x = (\text{Norm}, \text{I.C.\ param}, \tau_0, \eta/s, \tau_\Pi).
\end{equation*}
Presumably there exists a \emph{true} set of parameters $\xs$; the task now is to find the probability distribution of $\xs$ given the training data $(X, Y)$ and experimental measurements $\yexp$.
This distribution may be framed in terms of Bayes' theorem as
\begin{equation}
  P(\xs|X,Y,\yexp) \propto P(X,Y,\yexp|\xs) P(\xs)
\end{equation}
where
\begin{itemize}
  \item $P(\xs)$ is the \emph{prior} probability which embodies initial knowledge of $\xs$;
  \item $P(X,Y,\yexp|\xs)$ is the likelihood:
    the probability of observing $(X, Y, \yexp)$ given a proposed value of $\xs$; and
  \item $P(\xs|X,Y,\yexp)$ is the \emph{posterior} probability for $\xs$ given the observations $(X, Y, \yexp)$.
    This is the probability distribution we wish to construct.
\end{itemize}
In general Bayes' theorem has a normalization constant which has been omitted since we are only concerned with relative probabilities.

The remainder of this section applies the methodology from \cite{OHagan:2006ba,Higdon:2008cmc,Higdon:2014tva} to calibrate the model and determine the posterior probability for $\xs$.

\subsection{MCMC}

The workhorse of any Bayesian calibration is Markov chain Monte Carlo (MCMC), a powerful and flexible method for directly sampling the posterior probability.
Perhaps the most common version is the Metropolis-Hastings algorithm, which generates a random walk through parameter space by accepting or rejecting steps based on the posterior probability.
For a large number of steps the samples of the random walk equilibrate to the posterior distribution.
We use the affine-invariant ensemble sampler for MCMC \cite{Goodman:2010en,FM:2013mc}, an alternative algorithm which uses a large ensemble of interdependent walkers.
Ensemble sampling notably has a much shorter autocorrelation time than Metropolis-Hastings sampling and hence converges  more quickly to the equilibrium distribution. 

We place a non-informative flat prior on $\xs$, that is, the prior probability is constant within the design range (table~\ref{tab:design}) and zero outside.
The likelihood is evaluated in principal component space:
at each MCMC-sampled $\xs$ point, the emulator predicts the principal components $\z(\xs)$, and the likelihood is
\begin{equation}
  P(\z|\xs) \propto \exp\biggl\{ -\frac{1}{2} [\z(\xs) - \zexp]\tran \Sigma_z^{-1} [\z(\xs) - \zexp] \biggr\},
  \label{eq:likelihood}
\end{equation}
where $\zexp$ is the principal component transform of the experimental data $\yexp$ and $\Sigma_z$ is the covariance (uncertainty) matrix.
There are a number of sources of uncertainty including experimental statistical and systematic error, model statistical and systematic error, and emulator uncertainty.
In the present study, we do not attempt to precisely account for each contribution, for this would inevitably require dubious assumptions about systematic error correlations and the unknown error of the model.
We assume a simple fractional error on the principal components, i.e.\ the covariance matrix is
\begin{equation}
  \Sigma_z = \text{diag}(\sigma^2_z\,\zexp),
\end{equation}
where $\sigma^2_z$ is a manually set constant ($\sigma_z = 0.06$ for the present study).
While this is itself a rough assumption, it is perhaps no worse than the alternative, since experimental systematic errors are typically estimated percentages themselves and the principal component transformation automatically includes natural correlations among observables.
The primary goal of this study is to develop and test a model-to-data comparison framework; details such as the precise treatment of uncertainties can be improved later.

Given the flat prior on $\xs$, the posterior probability is equal to the likelihood \eqref{eq:likelihood} within the design range and zero outside.
The MCMC algorithm draws samples directly from this distribution.
We first run approximately $1 \times 10^6$ MCMC steps to allow the chain to equilibrate---these ``burn-in'' samples are discarded---followed by approximately $3 \times 10^6$ steps to generate the posterior distribution.
\todo{Remember to update these numbers for the final version.}

\subsection{Results}

\cite{Abelev:2014mda}
\cite{Shen:2011zc,Heinz:2011kt}

Figures~\ref{fig:cal_post_glb} and \ref{fig:cal_post_kln}.
Table~\ref{tab:posterior}.
Figure~\ref{fig:post_draws}.

\todo{
  Posterior boxplots?
  Goodness of fit visualization?
}

\widefig{cal_post_glb}{
  Posterior marginal and joint distributions of the calibration parameters for the Glauber model.
  On the diagonal are histograms of MCMC samples for the respective parameters,
  on the lower triangle are two-dimensional histograms of MCMC samples showing the correlation between pairs of parameters,
  and on the upper triangle are approximate contours for 68\%, 95\%, and 99\% confidence regions along with a dot indicating the median.
}

\widefig{cal_post_kln}{
  Same as FIG.~\ref{fig:cal_post_glb} for the KLN model.
}

\colfig{post_compare}{
  Comparison of posterior distributions of $\eta/s$ for Glauber (blue) and KLN (green).
  These are the same histograms as in FIGS.~\ref{fig:cal_post_glb} and \ref{fig:cal_post_kln}, expanded and placed on the same axis.
  The vertical grey lines indicate the canonical values 0.08 for Glauber and 0.20 for KLN \cite{}.
}

\begin{table*}
  \caption{
    \label{tab:posterior}
    Quantitative summary of posterior distributions.
    For each parameter, the initially guessed value \todo{[cite somehow]}, mean, median, and confidence intervals are given.
  }
  \input{fig/posterior_table}
\end{table*}

\widefig{post_draws}{
  Random realizations of the calibrated posterior for Glauber (top, blue) and KLN (bottom, green) initial conditions.
  Similar to FIG.~\ref{fig:prior_draws},
  except the lines are posterior emulator predictions instead of explicit prior calculations.
}



\section{Conclusion}

Use \trento\ \cite{Moreland:2014oya}.


\section*{Acknowledgments}

We would like to thank Scott Pratt and Paul Sorensen for helpful discussions and valuable feedback.
This work would not have been possible without the foundations laid by the MADAI collaboration, funded through NSF grant no.~PHY-0941373, and the Statistical and Applied Mathematical Sciences Institute (SAMSI) program on massive datasets, funded through NSF grant no.~\todo{number}.
SAB is supported by the U.S. Department of Energy Grant no.~DE-FG02-05ER41367; JEB acknowledges support through grants NSF no.~PHY-0941373 and DOE no.~DE-FG02-05ER41367.
\todo{More\ldots}
This research was completed using over five million CPU hours provided by the Open Science Grid \cite{Pordes:2007zzb,Sfiligoi:2010zz}, which is supported by the National Science Foundation and the U.S.\ Department of Energy's Office of Science.
Several software packages were invaluable:
the Python MCMC toolkit \texttt{emcee} \cite{FM:2013mc} and Gaussian process library \texttt{george} \cite{Ambikasaran:2014gp}, and the general-purpose tool \texttt{GNU Parallel} \cite{Tange:2011pa}.
\todo{Github repositories?}


\appendix

\section{\label{app:train}Emulator training}

\widefig{train_glb}{
  Posterior distributions of the principal component Gaussian process hyperparameters for the Glauber model.
  The notation $\ell\;x$ means the squared-exponential correlation length for parameter $x$.
}

\widefig{train_kln}{
  Same as FIG.~\ref{fig:train_glb} for the KLN model.
}



\bibliography{mtd}


\end{document}
